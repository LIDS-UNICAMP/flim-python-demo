import os
from torch.utils.data import Dataset
from torchvision import transforms, utils
from skimage import io, transform
from skimage import color
from skimage import measure
import torch
import numpy as np
from pyflim import util

class FLIMData(Dataset):
    def __init__(self, orig_folder, marker_folder=None, images_list=None, test_images_list=None, label_folder=None, orig_ext=".png", marker_ext="-seeds.txt", label_ext=".png",transform=None, bits=8):
        """
        Arguments:
            orig_folder (string): Path to the folder with the original images.
            marker_folder (string): Path to the folder with the marker files for each training image.
            training_images_list (string or list): List of training images or path to the txt file with training images (if blank, all files within the orig folder will be considered).
            label_folder (string): Path to the folder with the ground-truth annotation for each image.
            orig_ext (string): File extension to be considered when looking at the orig_folder.
            marker_ext (string): Extension to append to the filename of the original final when looking at the marker_folder.
            transform (callable, optional): Optional transform to be applied
                on a sample.
            bits (int): Number of bits usage in color information, default is 8 (for 256 colors)
        """
        self.marker_folder = marker_folder
        self.orig_folder = orig_folder
        self.label_folder = label_folder

        if(marker_folder is not None):
            self.mode = "train"
        else:
            self.mode = "test"
        if(images_list is not None):
            self.file_list = FLIMData.get_file_list(images_list)
        else:
            self.file_list = util.readAllFilesFromFolder(orig_folder, orig_ext)

        if(self.mode == "train"):
            self.file_list = FLIMData.filter_from_folder(self.file_list, marker_folder, marker_ext)
            
        assert len(self.file_list) >= 1, "No training images were loaded"
        
        self.orig_ext = orig_ext
        self.marker_ext = marker_ext
        self.label_ext = label_ext
        self.transform = transform
        if(self.transform == None):
            self.batchable = False
        self.results = []
        self.ctb = self.get_blue_to_red_color_table(bits=bits)

    def test(self):
        self.mode = "test"

    def train(self):
        self.mode = "train"

    def filter_from_folder(read_list, marker_folder, marker_ext):
        if(marker_folder is None):
            return read_list
        index_to_remove = []
        for i,filename in enumerate(read_list):
            marker_file = marker_folder+filename.split(".")[0]+marker_ext
            if not os.path.isfile(marker_file):
                index_to_remove.append(i)
        filtered_list = [i for j, i in enumerate(read_list) if j not in index_to_remove]
        return filtered_list
        

    def get_file_list(file_list):
        if isinstance(file_list, str):
            return util.readFileList(file_list)
        elif isinstance(file_list, list):
            return  file_list
        else:
            assert False, "Image list parameter is neither a list nor a string"
            
    #This function is different than the one implemented in the LibIFT and provide worst results
    def rgb2labnorm(rbg_image):
        lab = color.rgb2lab(rbg_image)
        lab[:,:,0]/=100
        lab[:,:,1]=(lab[:,:,1]+87)/(87+99)
        lab[:,:,2]=(lab[:,:,2]+108)/(108+95)

        return lab

    def _labf(x):
        if x >= 8.85645167903563082e-3:
            return x ** (0.33333333333)
        else:
            return (841.0 / 108.0) * (x) + (4.0 / 29.0)
    
    def image_to_lab(image):
        return FLIMData._image_to_lab(image)      
    
    def get_blue_to_red_color_table(self, bits):
        n_colors=2**bits
        if bits == 8:
            img_dtype = np.uint8
        elif bits == 16:
            img_dtype = np.uint16
        colors = np.arange(0, n_colors).reshape(-1)
        colors = 4 * (colors/n_colors) + 1
        ctb = np.zeros((*colors.shape, 3)).astype(img_dtype)

        ctb[:, 0] = (n_colors - 1) * \
                  np.maximum(0, (3 - abs(colors - 4) - abs(colors - 5)) / 2)
        ctb[:, 1] = (n_colors - 1) * \
                  np.maximum(0, (4 - abs(colors - 2) - abs(colors - 4)) / 2)
        ctb[:, 2] = (n_colors - 1) * \
                  np.maximum(0, (3 - abs(colors - 1) - abs(colors - 2)) / 2)
      
        return ctb
    
    def gray_to_colored(self, gray_image):
        rgb_image = np.zeros((*gray_image.shape, 3))
        rgb_image[:, :, :] = self.ctb[gray_image]
      
        return rgb_image
      
    def _image_to_lab(image):
        image = image / image.max()
    
        labf_v = np.vectorize(FLIMData._labf)
    
        new_image = np.zeros_like(image)
        R, G, B = image[:, :, 0], image[:, :, 1], image[:, :, 2]
    
        X = (
            0.4123955889674142161 * R
            + 0.3575834307637148171 * G
            + 0.1804926473817015735 * B
        )
        Y = (
            0.2125862307855955516 * R
            + 0.7151703037034108499 * G
            + 0.07220049864333622685 * B
        )
        Z = (
            0.01929721549174694484 * R
            + 0.1191838645808485318 * G
            + 0.9504971251315797660 * B
        )
    
        X = labf_v(X / 0.950456)
        Y = labf_v(Y / 1.0)
        Z = labf_v(Z / 1.088754)
    
        new_image[:, :, 0] = 116 * Y - 16
        new_image[:, :, 1] = 500 * (X - Y)
        new_image[:, :, 2] = 200 * (Y - Z)
    
        # new_image = rgb2lab(image)
    
        new_image[:, :, 0] = new_image[:, :, 0] / 99.998337
        new_image[:, :, 1] = (new_image[:, :, 1] + 86.182236) / (86.182236 + 98.258614)
        new_image[:, :, 2] = (new_image[:, :, 2] + 107.867744) / (107.867744 + 94.481682)
    
        return new_image

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        image_path = os.path.join(self.orig_folder,self.file_list[idx])
        if(len(image_path.split(".")) == 1):
            image_path+=self.orig_ext
        
        image_data = io.imread(image_path)
        if len(image_data.shape) == 2:
            image_data = self.gray_to_colored(image_data)
        
        image = FLIMData.image_to_lab(image_data)
        original_size = np.array(image.shape)

        marker_label_image = None
        if(self.marker_folder is not None):
            marker_file = os.path.join(self.marker_folder,self.file_list[idx].split(".")[0]+self.marker_ext)
            marker_image = FLIMData.label_markers_by_component(FLIMData.get_marker_image(marker_file))
            marker_label_image = FLIMData.get_marker_image(marker_file)
        else:
            marker_image = None
            
        if(self.label_folder is not None):
            label_file = os.path.join(self.label_folder,self.file_list[idx]+self.label_ext)
            label = io.imread(label_file)
        else:
            label = None

        saliency = None
        if(len(self.results) >= idx+1):
            saliency=self.results[idx]

        sample = {'image': image, "image_path":image_path, 'original_size': original_size }

        if(label is not None):
            sample['label'] = label
        if(saliency is not None):
            sample['saliency'] = saliency
        if(marker_image is not None):
            sample['markers'] = marker_image
        if(marker_label_image is not None):
            sample['markers_label'] = marker_label_image
        
        if self.transform:
            sample = self.transform(sample)

        return sample


    def read_image_markers(marker_file):
        marker_list = []
        with open(marker_file) as reader:
            full_file = reader.readlines()
        for line in full_file:
            line_split = line.split(" ")
            if(len(line_split) == 3):
                n_markers = line_split[0]
                image_size = (int(line_split[2].split("\n")[0]), int(line_split[1]))
            else:
                marker_class = int(line_split[3].split("\\")[0])+1
                marker_list.append((int(line_split[1]),int(line_split[0]), marker_class))

        return marker_list, image_size

    def get_marker_image(marker_file):
        markers, image_size = FLIMData.read_image_markers(marker_file)
        marker_image = np.zeros((image_size[0], image_size[1]))
        for m in markers:
            marker_image[m[0], m[1]] = m[2]+1
        return marker_image

    def label_markers_by_component(marker_image):
        bin_image = np.copy(marker_image)
        bin_image[bin_image>0] = 1
        component_image = measure.label(bin_image, background=0)

        return component_image

    def save_images(self, output_folder):
        if not os.path.exists(output_folder): 
            os.makedirs(output_folder) 
        for sample in self:
            saliency = sample["saliency"]
            image_file = sample["image_path"].split("/")[-1]
            
            io.imsave(output_folder+"/"+image_file, saliency)
        
        
#-----------------------------------Transforms-----------------------------------------------
class Rescale(object):
    """Rescale the image in a sample to a given size.

    Args:
        output_size (tuple or int): Desired output size. If tuple, output is
            matched to output_size. If int, smaller of image edges is matched
            to output_size keeping aspect ratio the same.
    """

    def __init__(self, output_size):
        assert isinstance(output_size, (int, tuple))
        
        if isinstance(output_size, int):
            self.output_size = (output_size, output_size)
        else:
            self.output_size = output_size

    def __call__(self, sample):
        image, image_path = sample['image'], sample['image_path']
        label = None
        saliency = None
        markers = None
        if 'label' in sample:
            label = sample['label']
        if 'saliency' in sample:
            saliency = sample['saliency']
        if 'markers' in sample:
            markers = sample['markers']
        if 'markers_label' in sample:
            markers_label = sample['markers_label']

        h, w = image.shape[:2]
        new_h, new_w = self.output_size

        new_h, new_w = int(new_h), int(new_w)
        image_new = transform.resize(image, (new_h, new_w))
        if(markers is not None):
            scale = np.array([new_h, new_w]) / np.array([h,w])

            index_list = [(x,y) for x,y in np.transpose(np.where(markers>0))]
            new_index_list = [(x,y) for x,y in  np.array([scale*x for x in index_list]).astype(int)]
            markers_new = np.zeros((int(round(markers.shape[0]*scale[0])), int(round(markers.shape[1]*scale[1]))))
            markers_label_new = np.zeros((int(round(markers_label.shape[0]*scale[0])), int(round(markers_label.shape[1]*scale[1]))))
            for (x,y),(x_,y_) in zip(new_index_list, index_list):
                markers_new[x,y] = markers[x_,y_]
                markers_label_new[x,y] = markers_label[x_,y_]
        else:
            markers_new = None
            
        
        if(label is not None):
            label_new = transform.resize(label, (new_h, new_w))
        else:
            label_new = None

        out_sample = {'image': image_new, "image_path":image_path, 'original_size': sample['original_size']}
        
        if label is not None:
            out_sample['label'] = label
        if saliency is not None:
            out_sample['saliency'] = saliency
        if markers_new is not None:
            out_sample['markers'] = markers_new
            out_sample['markers_label'] = markers_label_new
            
        return out_sample
    
class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        
        image, image_path = sample['image'], sample['image_path']
        label = None
        saliency = None
        markers = None
        if 'label' in sample:
            label = sample['label']
        if 'saliency' in sample:
            saliency = sample['saliency']
        if 'markers' in sample:
            markers = sample['markers']
        if 'markers_label' in sample:
            markers_label = sample['markers_label']
        
        image_new = torch.from_numpy(image).permute((2, 0, 1)).float()
        if(label is not None):
            label_new = torch.unsqueeze(torch.from_numpy(label), 0)
        else:
            label_new = None
            
        
        out_sample = {'image': image_new, "image_path":image_path, 'original_size': sample['original_size']}
        
        if label is not None:
            out_sample['label'] = label
        if saliency is not None:
            out_sample['saliency'] = saliency
        if markers is not None:
            out_sample['markers'] = markers
            out_sample['markers_label'] = markers_label
            
        return out_sample
